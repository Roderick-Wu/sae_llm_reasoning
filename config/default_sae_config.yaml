# SAE Configuration File
# Configuration for training Sparse Autoencoders on reasoning features

# Model parameters
model_name: "Llama-3.1-8B"
model_dir: "/home/wuroderi/projects/def-zhijing/wuroderi/models"

# Data parameters  
dataset_dir: "/home/wuroderi/projects/def-zhijing/wuroderi/Linear_Reasoning_Features/dataset"
output_dir: "/home/wuroderi/projects/def-zhijing/wuroderi/steering_vs_sae/outputs"
hs_cache_path: "/home/wuroderi/projects/def-zhijing/wuroderi/Linear_Reasoning_Features/outputs"

# SAE Architecture
hidden_dim: 4096  # Llama-3.1-8B hidden dimension
expansion_factor: 4  # Dictionary size = hidden_dim * expansion_factor (16384 features)
layers_to_train: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]  # Focus on all layers

# Training parameters
batch_size: 256
learning_rate: 1e-4
num_epochs: 50
sparsity_penalty: 1e-3  # Adjust based on desired sparsity level
reconstruction_weight: 1.0
device: "cuda"

# Feature extraction
top_k_features: 10  # Number of top reasoning features to analyze
reasoning_threshold: 0.5  # Threshold for reasoning vs memory classification

# Logging
log_interval: 100
save_interval: 10
use_wandb: false
wandb_project: "sae-reasoning-features"